{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bbdf4d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from g2p_en import G2p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a06bf45",
   "metadata": {},
   "source": [
    "Author: Maksym Sarana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "388cb816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting g2p-en\n",
      "  Downloading g2p_en-2.1.0-py3-none-any.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting distance>=0.1.3\n",
      "  Downloading Distance-0.1.3.tar.gz (180 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.3/180.3 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.13.1 in /Users/maksym/anaconda3/lib/python3.10/site-packages (from g2p-en) (1.23.5)\n",
      "Requirement already satisfied: nltk>=3.2.4 in /Users/maksym/anaconda3/lib/python3.10/site-packages (from g2p-en) (3.7)\n",
      "Collecting inflect>=0.3.1\n",
      "  Downloading inflect-6.0.4-py3-none-any.whl (34 kB)\n",
      "Collecting pydantic>=1.9.1\n",
      "  Downloading pydantic-1.10.8-cp310-cp310-macosx_11_0_arm64.whl (2.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click in /Users/maksym/anaconda3/lib/python3.10/site-packages (from nltk>=3.2.4->g2p-en) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/maksym/anaconda3/lib/python3.10/site-packages (from nltk>=3.2.4->g2p-en) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in /Users/maksym/anaconda3/lib/python3.10/site-packages (from nltk>=3.2.4->g2p-en) (4.64.1)\n",
      "Requirement already satisfied: joblib in /Users/maksym/anaconda3/lib/python3.10/site-packages (from nltk>=3.2.4->g2p-en) (1.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/maksym/anaconda3/lib/python3.10/site-packages (from pydantic>=1.9.1->inflect>=0.3.1->g2p-en) (4.4.0)\n",
      "Building wheels for collected packages: distance\n",
      "  Building wheel for distance (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for distance: filename=Distance-0.1.3-py3-none-any.whl size=16257 sha256=0c00c65eaaf786d7624c5b1dab527cc0f8a090be7b03dcf455ea13d3a20da6a3\n",
      "  Stored in directory: /Users/maksym/Library/Caches/pip/wheels/6a/41/b0/39f403bd1fb459600f3ced3f74e8e88ffd5a409584a043785a\n",
      "Successfully built distance\n",
      "Installing collected packages: distance, pydantic, inflect, g2p-en\n",
      "Successfully installed distance-0.1.3 g2p-en-2.1.0 inflect-6.0.4 pydantic-1.10.8\n"
     ]
    }
   ],
   "source": [
    "! pip install g2p-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8205db74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "fb2739a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source: https://gist.github.com/proger/a7e820fbfa0181273fdbf2351901d0d8\n",
    "\n",
    "def make_frames(wav):\n",
    "    return torchaudio.compliance.kaldi.mfcc(wav)\n",
    "\n",
    "\n",
    "class LibriSpeech(torch.utils.data.Dataset):\n",
    "    def __init__(self, url='dev-clean'):\n",
    "        super().__init__()\n",
    "        self.librispeech = torchaudio.datasets.LIBRISPEECH('.', url=url, download=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.librispeech)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        wav, sr, text, speaker_id, chapter_id, utterance_id = self.librispeech[index]\n",
    "        return make_frames(wav), text\n",
    "\n",
    "      \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim=13, subsample_dim=128, hidden_dim=1024):\n",
    "        super().__init__()\n",
    "        self.subsample = nn.Conv1d(input_dim, subsample_dim, 5, stride=4, padding=3)\n",
    "        self.lstm = nn.LSTM(subsample_dim, hidden_dim, batch_first=True, num_layers=3, dropout=0.2)\n",
    "\n",
    "    def subsampled_lengths(self, input_lengths):\n",
    "        # https://github.com/vdumoulin/conv_arithmetic\n",
    "        p, k, s = self.subsample.padding[0], self.subsample.kernel_size[0], self.subsample.stride[0]\n",
    "        o = input_lengths + 2 * p - k\n",
    "        o = torch.floor(o / s + 1)\n",
    "        return o.int()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        x = self.subsample(x.mT).mT\n",
    "        x = x.relu()\n",
    "        x, _ = self.lstm(x)\n",
    "        return x.relu()\n",
    "\n",
    "class Recognizer(nn.Module):\n",
    "    def __init__(self, feat_dim=1024, vocab_size=55+1):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Linear(feat_dim, vocab_size)\n",
    "\n",
    "    def forward(self, features):\n",
    "        features = self.classifier(features)\n",
    "        return features.log_softmax(dim=-1)\n",
    "    \n",
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.g2p = G2p()\n",
    "\n",
    "        # http://www.speech.cs.cmu.edu/cgi-bin/cmudict\n",
    "        self.rdictionary = [\"ε\", # CTC blank\n",
    "                            \" \",\n",
    "                            \"AA0\", \"AA1\", \"AE0\", \"AE1\", \"AH0\", \"AH1\", \"AO0\", \"AO1\", \"AW0\", \"AW1\", \"AY0\", \"AY1\",\n",
    "                            \"B\", \"CH\", \"D\", \"DH\",\n",
    "                            \"EH0\", \"EH1\", \"ER0\", \"ER1\", \"EY0\", \"EY1\",\n",
    "                            \"F\", \"G\", \"HH\",\n",
    "                            \"IH0\", \"IH1\", \"IY0\", \"IY1\",\n",
    "                            \"JH\", \"K\", \"L\", \"M\", \"N\", \"NG\",\n",
    "                            \"OW0\", \"OW1\", \"OY0\", \"OY1\",\n",
    "                            \"P\", \"R\", \"S\", \"SH\", \"T\", \"TH\",\n",
    "                            \"UH0\", \"UH1\", \"UW0\", \"UW1\",\n",
    "                            \"V\", \"W\", \"Y\", \"Z\", \"ZH\"]\n",
    "\n",
    "        self.dictionary = {c: i for i, c in enumerate(self.rdictionary)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rdictionary)\n",
    "\n",
    "    def encode(self, text):\n",
    "        labels = [c.replace('2', '0') for c in self.g2p(text) if c != \"'\"]\n",
    "        targets = torch.LongTensor([self.dictionary[phoneme] for phoneme in labels])\n",
    "        return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8ad8f35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LibriSpeech()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ce60cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  247M  100  247M    0     0  3587k      0  0:01:10  0:01:10 --:--:-- 4655k 0:00:16 5734k 3658k      0  0:01:09  0:00:56  0:00:13 4959k\n"
     ]
    }
   ],
   "source": [
    "! curl https://wilab.org.ua/lstm_p3_360+500.pt -o lstm_p3_360+500.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e17790c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocabulary()\n",
    "encoder = Encoder()\n",
    "recognizer = Recognizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "719711ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = torch.load('lstm_p3_360+500.pt', map_location='cpu')\n",
    "encoder.load_state_dict(ckpt['encoder'])\n",
    "recognizer.load_state_dict(ckpt['recognizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "459e2f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_array(array, separators):\n",
    "    start_indexes = []\n",
    "    splits = [[]]\n",
    "    \n",
    "    for i, x in enumerate(array):\n",
    "        if (x in separators) and splits[-1]:\n",
    "            splits.append([])\n",
    "            \n",
    "        if (x not in separators):\n",
    "            if not splits[-1]:\n",
    "                start_indexes.append(i)\n",
    "            splits[-1].append(x)\n",
    "            \n",
    "    return splits if splits[-1] else splits[:-1], start_indexes\n",
    "    \n",
    "def remove_duplicates(array):\n",
    "    res = []\n",
    "    for x in array:\n",
    "        if res and x == res[-1]:\n",
    "            continue\n",
    "        res.append(x)\n",
    "        \n",
    "    return res\n",
    "\n",
    "def get_alignments(audio_frames, frames_per_second=25.0):\n",
    "    features = encoder(audio_frames)\n",
    "    outputs = recognizer.forward(features)\n",
    "    utterance_symbol_indexes = torch.argmax(outputs, dim=1)\n",
    "\n",
    "    splits, start_indexes = split_array(utterance_symbol_indexes.numpy(), {0, 1})\n",
    "        \n",
    "    alignments = []\n",
    "    for split, start_index in zip(splits, start_indexes):\n",
    "        alignments.append({\n",
    "            'start_time': start_index / frames_per_second,\n",
    "            'end_time': (start_index + len(split)) / frames_per_second,\n",
    "            'phones': ' '.join([vocab.rdictionary[x] for x in remove_duplicates(split)])\n",
    "        })\n",
    "        \n",
    "    return alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c33c7beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start_time': 0.8, 'end_time': 1.0, 'phones': 'M IH1 S T ER0'},\n",
       " {'start_time': 1.08, 'end_time': 1.32, 'phones': 'K R IH1 L T'},\n",
       " {'start_time': 1.36, 'end_time': 1.4, 'phones': 'ER0'},\n",
       " {'start_time': 1.52, 'end_time': 1.6, 'phones': 'IH1 Z'},\n",
       " {'start_time': 1.64, 'end_time': 1.72, 'phones': 'DH AH0'},\n",
       " {'start_time': 1.84, 'end_time': 1.88, 'phones': 'AH0'},\n",
       " {'start_time': 1.92, 'end_time': 2.0, 'phones': 'P'},\n",
       " {'start_time': 2.04, 'end_time': 2.12, 'phones': 'AA1 S'},\n",
       " {'start_time': 2.16, 'end_time': 2.32, 'phones': 'AH0 L'},\n",
       " {'start_time': 2.4, 'end_time': 2.48, 'phones': 'AH1 V'},\n",
       " {'start_time': 2.52, 'end_time': 2.6, 'phones': 'DH AH0'},\n",
       " {'start_time': 2.64, 'end_time': 2.84, 'phones': 'M IH1 D AH0 L'},\n",
       " {'start_time': 2.92, 'end_time': 3.16, 'phones': 'K L AE1 S'},\n",
       " {'start_time': 3.2, 'end_time': 3.24, 'phones': 'AH0'},\n",
       " {'start_time': 3.32, 'end_time': 3.36, 'phones': 'Z'},\n",
       " {'start_time': 3.52, 'end_time': 3.64, 'phones': 'AH0 N D'},\n",
       " {'start_time': 3.72, 'end_time': 3.88, 'phones': 'W IH1 R'},\n",
       " {'start_time': 3.96, 'end_time': 4.12, 'phones': 'G L AE1'},\n",
       " {'start_time': 4.16, 'end_time': 4.2, 'phones': 'D'},\n",
       " {'start_time': 4.32, 'end_time': 4.36, 'phones': 'T'},\n",
       " {'start_time': 4.4, 'end_time': 4.44, 'phones': 'UW1'},\n",
       " {'start_time': 4.48, 'end_time': 4.68, 'phones': 'W EH1 L K'},\n",
       " {'start_time': 4.72, 'end_time': 4.8, 'phones': 'AH0 M'},\n",
       " {'start_time': 4.92, 'end_time': 5.04, 'phones': 'HH IH1 Z'},\n",
       " {'start_time': 5.16, 'end_time': 5.2, 'phones': 'G'},\n",
       " {'start_time': 5.24, 'end_time': 5.44, 'phones': 'AA1 S P'},\n",
       " {'start_time': 5.48, 'end_time': 5.56, 'phones': 'AH0 L'}]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_alignments(dataset[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47009cbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c4f120",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c53420",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f58aa0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
